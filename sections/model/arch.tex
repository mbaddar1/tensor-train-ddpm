%! Author = mbaddar
%! Date = 4/23/24

% Preamble
\documentclass[11pt]{article}

% ref docs
% 1. David GM work https://arxiv.org/pdf/2402.15285 , sec 3 for FTT
% Packages

% Document
\begin{document}
%    \subsubsection{Notation}
%    This table describes the notation used in this work:
%    % similar to Kolda and Bader paper https://www.kolda.net/publication/TensorReview.pdf
%    \begin{center}
%        \begin{tabular}{ |c|c| }
%            \hline
%            x            & a Lowercase letter denotes a real-valued scalar : $x\in \mathbb{R}$                                                        \\
%            $\mathbf{x}$ & a Lowercase boldface letter represents a real-valued vector : $\mathbf{x} \in \mathbb{R}^{D}$                              \\
%            X            & an Uppercase letter represents a real-valued Matrix : $X \in \mathbb{R}^{d_1 \times d_2}$                                  \\
%            $\mathbf{X}$ & an Uppercase boldface letter denotes either a real-valued Tensor or order $N$ or the low-rank tensor-train representation.
%            It will be explicitly stated $\mathbf{X} \in \mathbb{R}^{\bigtimes_{i=1}^{N} d_i}$
%            \hline
%        \end{tabular}
%    \end{center}
    Given a sample from the training data $\mathbf{x}_0 \in q(\mathbf{x}_0)$,the main optimization task we will study is:
    \begin{equation}
        \begin{aligned}
            \mathcal{L}_{\text {simple }}(\bm{\theta})) =\mathbb{E}_{t,\mathbf{x}_t \mid \mathbf{x}_0
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
                {\bm{\theta}}\left(\mathbf{x}_t, t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple-repeated}
    \end{equation}

    Where
    \begin{align*}
        \mathbf{x}_t &\in \mathbb{R}^D \\
        t &\in \mathbb{Z}^{+}\\
        \boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right) &: \mathbb{R}^{D} \times \mathbb{R} \rightarrow \mathbb{R}^{D}\\
        \bm{\epsilon} &\sim \mathcal{N}(\mathbf{0},\mathbf{I}),\mathbf{0} \in \mathbb{R}^D, \mathbf{I} \in \mathbb{ R}^{D \times D} \\
        \mathbf{x}_t &=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}\\
        t &\sim \mathcal{U}(1,T)\\
    \end{align*}
    See eq.(\ref{eq:ddpm-constants},\ref{eq:ddpm-xt-given-xt-minus-1},\ref{eq:xt-give-x0-eqn-reparm}) for more details.


    Let $\bm{\mathscr{A}} \in \mathbb{R}^{\prod_{i=1}^{D}m}$ be a tensor-train of order $D$ whose elements can be written as
    \begin{equation}
        % similar to eq. (3) in https://arxiv.org/pdf/2108.00089
        \label{eq:tt}
        \begin{aligned}
            \bm{\mathscr{A}} &= G_1[.,i_1,.]G_2[.,i_2,.]\dots G_D[.,i_D,.]
        \end{aligned}
    \end{equation}
    Where each core $G_i$ is a 3-order real matrix: $G_i \in \mathbb{R}^{r_{i-1}\times m_i \times r_{i}}$ and $r_0=r_D=1$.

    There are many TT-based approaches for density estimation in literature \cite{han2024tensor,novikov2022tensortrain}.
    This architecture is based on the TTDE model introduced in \cite{novikov2022tensortrain}.
    However, the core difference is the TTDE directly approximates the density $q_{\theta}(\mathbf{x}_0)$, but on the other hands,
    our work approximates $\bm{\epsilon}_{\bm{\theta}}(\mathbf{x}(t),t)$ by minimizing the $L2$ loss function in
    eq(\ref{eq:ddpm-loss-simple}). Minimizing the $L2$ loss is a proxy for maximizing the log-likelihood
    $\log q_{\theta}(\mathbf{x}_0)$.

    Then the proposed FTT approximation model for $\boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right)$ can be formulated as:
    \begin{subequations}
        \begin{align}
            \mathbf{u}(t)&= \mathbf{x}(t) + \mathbf{t}^{\prime} \\
            y_{i}(t) &= \mathbf{A}\bm{\Phi}(\mathbf{u}(t)) \\
            &= \sum_{i_1=1,i_2=1,\dots,i_D=1}^{m,\dots,m} G_1[.,i_1,.]G_2[.,i_2.] \dots G_D[.,i_D,.]\bm{\phi}_1(u_1(t))\bm{\phi}_2(u_2(t))...\bm{\phi}_D(u_D(t))\\
            \boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right) &= \mathbf{y}(t)=[y_i(t)]_{i=1}^{D}
        \end{align}
        \label{eq:ftt-ddpm-noise-model}
    \end{subequations}
    Where $\mathbf{u}$ is the time-embedded image for latent variable $\mathbf{x}(t)$ by adding it to the Positional Encoding time.
    $\mathbf{t}^{\prime}$.


    We apply dimensional-decomposition for both input $\mathbf{u}(t)$ and output $\mathbf{y}(t)$.
    For input, we apply a tensor-valued basis function $\bm{\Phi}(\mathbf{u}(t))$ to introduce non-linearity in the model.
    The tensor basis $\bm{\Phi}(\mathbf{u}(t))$ is decomposed to the tensor product of a set of rank-1 basis function:
    \begin{align*}
        \bm{\Phi}(\mathbf{u}(t)) &= \bm{\phi_1}\otimes(u_1(t)) \otimes \bm{\phi_2}(u_2(t)) \dots \otimes \bm{\phi_D}(u_D(t))
    \end{align*}
    Where $\bm{\Phi}(\mathbf{u}(t)) \in : \mathbb{R}^{D} \rightarrow \mathbb{R}^{m_1 \times m_2 \dots \times m_D}$
    and $\bm{\phi}_i(u_i) \in \mathbb{R} \rightarrow \mathbb{R}^{m_i}$.

    \subsubsection{Basis Functions}
    Many classes of basis functions have been applied to FTT-approximations like B-Splines, Fourier Series,
    \cite{novikov2022tensortrain} Legendre Polynomials\cite{sommer2024generative}.
    As the proposed architecture is based on the TTDE model in \cite{novikov2022tensortrain},
    we focus on B-Splines basis function.

    \paragraph{B-Splines Basis Function} A B-spline function has two parameters: order $p$ and number of knots $K$.
    The function at $B(x_d;k,p) \in \mathbb{R} \rightarrow \mathbb{R}$ each knot with index $k=1,2,\dots,K$can be written as:
    \begin{equation}
        \begin{aligned}
            & B(x;k,0)=
            \begin
            {cases}
                1 & \text { if } s_k \leq x<s_{k+1} \\ 0 & \text { otherwise }
            \end{cases} \\ & B(x;k,p)=\frac{x-s_k}{s_{k+p}-s_k} B(x;k,p-1)+\frac{s_{k+p+1}-x}{s_{k+p+1}-s_{k+1}} B(x;k+1, p-1)
        \end{aligned}\label{eq:b-splines-basis}
    \end{equation}
    \textit{How B-splines are used as basis functions?}
    each $\bm{\phi}_i(u_i)$ is modeled as B-splines function with $m_i$ knots which are uniformly distributed over $u_i$'s support.
    The degree is fixed to $p=2$\cite{novikov2022tensortrain}.


    Accordingly, we can write the B-Splines realization of the rank-1 basis $\bm{\phi}_i(u_i(t))$ as follows:\\
    Let
    \begin{align*}
        u_i &\in [u_{i,min},u_{i,max}] \quad \forall i=1,2,\dots D  \text{ Support for } u_i\\
        \Delta &=\lfloor \frac{u_{i,max}-u_{i,min}}{m_i} \rfloor \text{ Step for the knots }\\
        s_{k} &= u_{i,min} + (k-1)\Delta \quad \forall k=1,2,\dots,m_i \text{ Knots values }\\
    \end{align*}
    Then the basis function $\bm{\phi}_i(u_i)$ can be formulated as:

    \begin{equation}
        \label{eq:b-splines-basis-ftt}
        \begin{aligned}
            \bm{\phi}_i(u_i) &= [\phi_i^k(u_i)]_{k=1}^{m_i} \quad \bm{\phi}_i(u_i) : \mathbb{R} \rightarrow
                \mathbb{R}^{m_i},\phi_i^k(u_i) : \mathbb{R} \rightarrow \mathbb{R}\\
            \phi_i^k(u_i) &= B(u_i;k,p)\\
            B(u_i;k,0)&=
            \begin
            {cases}
                1 & \text { if } s_k \leq x<s_{k+1} \\ 0 & \text { otherwise }
            \end{cases} \\
            B(u_i;k,p)&=\frac{u_i-s_k}{s_{k+p}-s_k} B(u_i;k,p-1)+\frac{s_{k+p+1}-u_i}{s_{k+p+1}-s_{k+1}} B(u_i;k+1, p-1) \quad \forall k=1,2,\dots,m_i
        \end{aligned}
    \end{equation}

\end{document}