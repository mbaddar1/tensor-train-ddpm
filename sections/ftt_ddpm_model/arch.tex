%! Author = mbaddar
%! Date = 4/23/24

% Preamble
\documentclass[11pt]{article}

% ref docs
% 1. David GM work https://arxiv.org/pdf/2402.15285 , sec 3 for FTT
% Packages

% Document
\begin{document}

    Given a sample from the training data $\mathbf{x}(0) \in p(\mathbf{x}(0))$,the main DDPM loss function under study is:
    \begin{equation}
        \begin{aligned}
            \mathcal{L}(\bm{\theta}) =\mathbb{E}_{t,\mathbf{x}(t) \mid \mathbf{x}_0,
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
                {\bm{\theta}}\left(\mathbf{x}(t), t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple-repeated}
    \end{equation}

    Where
    \begin{align*}
        \mathbf{x}(t) &\in \mathbb{R}^D \quad \forall t=0,1,2,\dots,T\\
        \boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right) &: \mathbb{R}^{D} \times \mathbb{R} \rightarrow \mathbb{R}^{D}\\
        \bm{\epsilon} &\sim \mathcal{N}(\mathbf{0},\mathbf{I}),\mathbf{0} \in \mathbb{R}^D, \mathbf{I} \in \mathbb{ R}^{D \times D} \\
        \mathbf{x}(t) &=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}\\
        t &\sim U(1,T)\\
    \end{align*}
    See eq.(\ref{eq:ddpm-constants},\ref{eq:ddpm-xt-given-xt-minus-1},\ref{eq:xt-give-x0-eqn-reparm}) for more details.


%    Let $\bm{A} \in \mathbb{R}^{m_1 \times m_2 \times \dots m_i \dots m_D }$ be a tensor-train of order $D$ where
%    each tensor element $\bm{A}[k_1,k_2,\dots,k_i,\dots,k_D],\forall k_i=1,2,\dots,m_i \text{and} i=1,2,\dots,D$
%    can be written as
%    \begin{equation}
%        % similar to eq. (3) in https://arxiv.org/pdf/2108.00089
%        \label{eq:tt}
%        \begin{aligned}
%            \bm{A}[k_1,k_2,\dots,k_i,\dots,k_D] &= \\
%            &\sum_{s_1,s_2,\dots,s_i,\dots,s_{i-1}}^{r_1,r_2,\dots,r_i,\dots,r_{D-1}}G_1[s_0,k_1,s_1]G_2[s_1,k_2,s_2]
%            \dots G_{i}[s_{i-1},k_i, s_i] \dots G_D[s_{D-1},k_D, s_{D}]
%        \end{aligned}
%    \end{equation}
%    Where each core $G_i$ is a 3-order real matrix: $G_i \in \mathbb{R}^{r_{i-1}\times m_i \times r_{i}}$ and $r_0=r_D=1$.


    There are many TT-based approaches for density estimation in literature\cite{han2024tensor,novikov2022tensortrain}.
    This proposed architecture in \eqref{eq:ftt-ddpm-noise-model} based on the TTDE model introduced in
    \cite{novikov2022tensortrain}.
    However, the core difference is the TTDE directly approximates the density $q_{\theta}(\mathbf{x}(0))$.
    On the other hand,our work approximates $\bm{\epsilon}_{\bm{\theta}}(\mathbf{x}(t),t)$ by minimizing the
    $\mathcal{L}_2$ loss function in eq(\ref{eq:ddpm-loss-simple}), which is a proxy for maximizing
    the log-likelihood $\log q_{\theta}(\mathbf{x}_0)$.

    Given the formulation of FTT in \eqref{eq:ftt},then the proposed FTT ansatz for
    $\boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right)$ can be formulated as follows:


    Let:
    \begin{equation}
        \label{eq:ftt-ddpm-formulation-givens}
        \begin{aligned}
            \mathbf{u}(t)&= \mathbf{x}(t) + \mathbf{g}(t) \\
            \boldsymbol{\epsilon}_{\bm{\theta}}\left(\mathbf{x}_t, t\right) &= \mathbf{y}(t)=[y_j(t)]_{j=1}^{D}
        \end{aligned}
    \end{equation}
    Then each component $y_j(t) \in \mathbb{R},j=1,2,\dots,D$ can be written as an FTT ansatz, by replacing $\bm{x}$ with $\bm{u}(t)$
    in \eqref{eq:ftt}
    % subequations : https://tex.stackexchange.com/a/358700

    \begin{subequations}
        \label{ftt-ddmp-noise-model}
        \begin{align}
            y_j(t) &= \mathbf{A}\bm{\Phi}(\mathbf{u}(t))\\
            \mathbf{A} &= G_1 G_2 \dots G_i \dots G_D\\
            \bm{\Phi}(\bm{u}(t)) &= \bigotimes_{i=1}^{D} \bm{\phi}_i(\bm{u}_i(t))
        \end{align}
    \end{subequations}


    Where
    \begin{itemize}
        \item $\mathbf{g}(t)$ is the positional-encoding function as in \eqref{eq:positional_encoding}
        \item $\mathbf{u}(t)$ is the time-embedded image for latent variable $\mathbf{x}(t)$ by adding it to the positional-encoded time $\mathbf{g}(t)$.
        \item $G_i \in \mathbb{R}^{r_{i-1} \times m_i \times r_i}$ is the $i^{\textit{th}}$ core of the tensor-train operator $\bm{A}$
    \end{itemize}
    See \eqref{eq:ftt} for the details of FTT-approximations.
    %%%
    \subsubsection*{Basis Functions}
    Many classes of basis functions have been applied to FTT-approximations like B-Splines, Fourier Series,
    \cite{novikov2022tensortrain} and Legendre Polynomials\cite{sommer2024generative}.
    Initially, we focus on applying two classes of basis functions: Legendre Polynomials and B-Splines.
    The rationale behind this selection is that both have been successfully applied in the domain of generative modeling
    \cite{novikov2022tensortrain,sommer2024generative}.
    %

    \paragraph{Legendre Polynomials}
    The Bonnel's recursion formula for Legendre-Polynomials can be written as
    \begin{equation}
        \label{eq:legendre-poly-recursion}
        \begin{aligned}
            P_{k}(x)&=
            \begin{aligned}
                \begin{cases}
                    X^k & k=0,1 \\
                    \frac{(2 k+1)x P_{k-1}(x)-k P_{k-2}(x)}{(k + 1)} & k=2,3,\dots \\
                \end{cases}
            \end{aligned}
        \end{aligned}
    \end{equation}
    Accordingly the basis function $\bm{\phi}(u_i(t)) : \mathbb{R} \rightarrow \mathbb{R}^{m_i}$ can be defined as
    \begin{equation}
        \label{eq:ftt-basis-legendre-polynomials}
        \begin{aligned}
            \bm{\phi}_i(u_i(t))&=[\phi_i^k(u_i(t))]_{k=0}^{m_i-1} \\
            \phi_i^k(u_i(t)) &= P_{k}(u_i(t))
        \end{aligned}
    \end{equation}
    Where $\bm{u}(t)=[u_i(t)]_{i=1}^{D}$

    \paragraph{B-Splines Basis Function} A B-spline function has two parameters: order $p$ and number of knots $K$.
    The function at $B(x;k,p) \in \mathbb{R} \rightarrow \mathbb{R}$ for each knot with index
    $k=1,2,\dots,K$ can be written as:
    \begin{equation}
        \begin{aligned}
            & B(x;k,0)=
            \begin
            {cases}
                1 & \text { if } o_k \leq x<o_{k+1} \\ 0 & \text { otherwise }
            \end{cases} \\ & B(x;k,p)=\frac{x-o_k}{o_{k+p}-o_k} B(x;k,p-1)+\frac{o_{k+p+1}-x}{o_{k+p+1}-o_{k+1}} B(x;k+1, p-1)
        \end{aligned}\label{eq:b-splines-basis}
    \end{equation}
    \textit{How B-splines are used as basis functions?}
    Each $\bm{\phi}_i(u_i)$ is modeled as B-splines function with $m_i$ knots which are uniformly distributed over $u_i$'s support.
    The degree is fixed to $p=2$\cite{novikov2022tensortrain}.


    Accordingly, we can write the B-Splines realization of the rank-1 basis
    $\bm{\phi}_i(u_i(t)) : \mathbb{R} \rightarrow \mathbb{R}^{m_i}$ as follows:


    Let
    \begin{align*}
        \mathbf{u} &=[u_i]_{i=1}^{D}\\
        u_i &\in [u_{i,min},u_{i,max}] \quad \forall i=1,2,\dots D  \text{ (Support for $u_i$) }\\
        \Delta &=\lfloor \frac{u_{i,max}-u_{i,min}}{m_i} \rfloor \text{ (Step for the knots) }\\
        o_{k} &= u_{i,min} + (k-1)\Delta \quad \forall k=1,2,\dots,m_i \text{ (Knots values) }\\
    \end{align*}
    Then the basis function $\bm{\phi}_i(u_i)$ can be formulated as:

    \begin{equation}
        \label{eq:b-splines-basis-ftt}
        \begin{aligned}
            \bm{\phi}_i(u_i) &= [\phi_i^k(u_i)]_{k=1}^{m_i} \quad \bm{\phi}_i(u_i) : \mathbb{R} \rightarrow
            \mathbb{R}^{m_i},\phi_i^k(u_i) : \mathbb{R} \rightarrow \mathbb{R}\\
            \phi_i^k(u_i) &= B(u_i;k,p)\\
%            B(u_i;k,0)&=
%            \begin
%            {cases}
%                1 & \text { if } o_k \leq x<o_{k+1} \\ 0 & \text { otherwise }
%            \end{cases} \\
%            B(u_i;k, p)&=\frac{u_i-s_k}{s_{k+p}-s_k} B(u_i;k, p-1)+\frac{s_{k+p+1}-u_i}{s_{k+p+1}-s_{k+1}} B(u_i;k+1, p-1) \quad \forall k=1,2,\dots,m_i
        \end{aligned}
    \end{equation}

\end{document}