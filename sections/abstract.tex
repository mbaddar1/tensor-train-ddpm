%! Author = mbaddar
%! Date = 4/23/24

% Preamble
\documentclass[11pt]{article}

% Sample Abstracts
% -- Comparative Analysis Papers --
% 1. https://arxiv.org/pdf/2311.13471
% 2. https://arxiv.org/pdf/2307.09809
% 3. https://arxiv.org/pdf/1904.04326


% Packages

% Document
\begin{document}
    Denoising Diffusion Probabilistic Models(DDPMs) are one of the recently developed class of Generative Models.
    Yet to our knowledge, there is no comparative analysis for different classes of the applied function approximator,
    along with their optimizers, which can justify the current model setups.
    Furthermore, all experimentation work has been exclusively focusing on image-generation, neglecting high-dimensional
    data from other domains.

%    The core task in DDPMs is finding the optimal functional approximator for $\bm{\epsilon}_{\theta}(\mathbf{x}(t),t)$
%    where $\bm{\theta}^{*} = argmin_{\bm{\theta}}\mathbb{E}_{t,\mathbf{x}(t),\bm{\epsilon}}\left(  \left\lVert \bm{\epsilon}-\bm{\epsilon}_{\theta}(\mathbf{x}(t),t)\right\lVert_2^2\right)$

    In this article, we will study the application of different Functional-Tensor-Trains(FTT) based approximators
    with Alternating Least Minimization (ALS) and Riemannian Optimization (RO) as the optimization algorithms.
    The study will compare these approximator to Neural Networks(NN) approximators optimized by Stochastic Gradient Descent(SGD).


    \textbf{The first goal} is to comprehensively analyze DDPM's convergence and quality's sensitivity to architecture and
    optimization parameters over different toy and real world high-dimensional datasets from different domains.
    As a result, \textbf{the second goal} is to introduce a FTT-DDPM model which is more efficient than NN-DDPM counterparts.

\end{document}