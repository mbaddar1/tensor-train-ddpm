%! Author = mbaddar
%! Date = 4/23/24

% Preamble
\documentclass[11pt]{article}

% Sample Abstracts
% -- Comparative Analysis Papers --
% 1. https://arxiv.org/pdf/2311.13471
% 2. https://arxiv.org/pdf/2307.09809
% 3. https://arxiv.org/pdf/1904.04326


% Packages

% Document
\begin{document}
    Score-based Generative Models are one of the recently developed class of Generative Models.
    Yet, to our knowledge, there is no much work on analyzing the convergence of these models for different classes of density functions.
    Furthermore, there is no comparative analysis for different architectures of function approximator, along with their corresponding
    optimizers, which can justify the current model setups.

    In this article, we focus on a simplified discrete version of Score-Based Generative Models, called
    Denoising Diffusion Probabilistic Models(DDPM).

    The core task in DDPMs is finding the optimal functional approximator for $\bm{\epsilon}_{\theta}(\mathbf{x}(t),t)$
    where $\bm{\theta}^{*} = argmin_{\bm{\theta}}\mathbb{E}_{t,\mathbf{x}(t),\bm{\epsilon}}\left(  \left\lVert \bm{\epsilon}-\bm{\epsilon}_{\theta}(\mathbf{x}(t),t)\right\lVert_2^2\right)$


    In literature, different Neural-Network (NN) architecture like ResNet and UNet are applied along with Gradient
    Descent as the optimization algorithm.

    In this article, we will study the application of different Functional-Tensor-Trains(FTT) based approximators
    with Alternating Least Minimization (ALS) and Riemannian Optimization (RO) as the optimization algorithms.
    The study will compare these approximator to NN-Gradient Descent counterparts.
    \textbf{The primary goal} is to comprehensively analyze DDPM's convergence and its sensitivity to architecture and
    optimization parameters.
    The analysis will include experimentation with toy and real-world densities.

    As an outcome of this analysis, the \textbf{secondary goal} is to introduce a FTT-DDPM model which
    is more efficient than NN-DDPM counterparts.


\end{document}