%! Author = mbaddar
%! Date = 4/25/24

% Preamble
\documentclass[11pt]{article}
% Packages


% Document
\begin{document}
    Score-based Generative models are one of the state-of-art models in generative models literature \cite{song2020generative,song2021scorebased}.
    Denoising Diffusion Probabilistic Models (DDPM) can be thought of as the simple discrete counterpart of score-based models.


    Given a random variable,  $\mathbf{x}(0) \in \mathbb{R}^{D} \sim p(\mathbf{x}_0)$ the model's task is to find a set
    of parameters $\bm{\theta}$ for the approximate density function $q(\mathbf{x}(0);\bm{\theta}) \subset \mathcal{Q}$ such
    that $q(\mathbf{x}(0);\bm{\theta}) \approx p(\mathbf{x}(0))$.
    More formally, the objective is to maximize the log-likelihood $ \log(q(\mathbf{x}(0);\bm{\theta}))$
    where $\boldsymbol{x} \sim p(\mathbf{x}(0))$.

    \subsection{Forward and Reverse Process}\label{subsec:forward-and-reverse-process}
    In DDPM, the training happens in two phases:

    \paragraph{Forward Process}
    In this phase, a set of intermediate latent variables are generated $\mathbf{x}(t), t={1,2,..,T}$, where
    $\mathbf{x}(0) \sim p(\mathbf{x}(0))$ is the input data sample, and $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})
    \quad \mathbf{0} \in \mathbb{R}^{D}, \mathbf{I} \in \mathbb{R}^{D \times D}$ is noisy seed data.\par


    Let:

    \begin{equation}
        \begin{aligned}
            \beta_t &, \quad 0 \leq \beta_t \leq 1,\beta \in \mathbb{R} \\
            \alpha_t &= 1-\beta_t \\
            \bar{\alpha}_t &= \prod_{i=1}^{T} \alpha_i \\
        \end{aligned}
        \label{eq:ddpm-constants}
    \end{equation}
    The latent variable $\mathbf{x}(t)$ has the following distribution:
    \begin{equation}
        \label{eq:ddpm-xt-given-xt-minus-1}
        \begin{aligned}
            q\left(\mathbf{x}(t) \mid \mathbf{x}(t-1)\right) &=\mathcal{N}\left(\mathbf{x}(t) ;
            \sqrt{1-\beta_t} \mathbf{x}(t-1), \beta_t \mathbf{I}\right) \\
            q\left(\mathbf{x}(1: T) \mid \mathbf{x}(0)\right) &=\prod_{t=1}^T q\left(\mathbf{x}(t) \mid \mathbf{x}(t-1)\right)
        \end{aligned}
    \end{equation}
    Which can be rewritten as

    \begin{equation}
        \label{eq:ddpm-xt-dist-given-x0}
        \begin{aligned}
            q\left(\mathbf{x}(t) \mid \mathbf{x}(0)\right)=\mathcal{N}\left(\mathbf{x}(t) ;
            \sqrt{\bar{\alpha}_t} \mathbf{x}(0),\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
        \end{aligned}
    \end{equation}


    \textcolor{red}{TBD : Add details for deriving the forward sampling equation}


    Accordingly, in the forward phase for each time $t=1,2,\dots T$, a corresponding $\mathbf{x}(t)$ is generated by
    sampling from density in eq.(\ref{eq:ddpm-xt-dist-given-x0}). In another word, $\mathbf{x}(t)$ can be sampled using
    the following equation (based on the Re-parameterization trick\cite{kingma2022autoencoding,TheRepar23:online})
    \begin{equation}
        \label{eq:xt-give-x0-eqn-reparm}
        \begin{aligned}
            \mathbf{x}(t) &=\sqrt{\bar{\alpha}_t} \mathbf{x}(0)+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}\\
        \end{aligned}
    \end{equation}
    Where $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
    %%%

    \paragraph{Reverse Process}
    This the process in which we try to train a model the reconstructs $\hat{\mathbf{x}}_0 \sim q_{\bm{\theta}}$ f
    rom normal noise $\mathbf{x}_T$, where $q_{\bm{\theta}}(\mathbf{x}(0)) \approx p(\mathbf{x}(0))$

    \newline
    \textcolor{red}{Add loss function derivation}
    \newline

    The loss function for DDPM, given a sample $\mathbf{x} \sim q(\mathbf{x}_0)$ can be written as:
    \begin{equation}
        \begin{aligned}
            L_{\text {simple }}(\theta) =\mathbb{E}_{t,\mathbf{x}(t) \mid \mathbf{x}_0,
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
            \theta\left(\mathbf{x}(t), t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple}
    \end{equation}
    Where $\mathbf{x}(t)$ is generated as in \eqref{eq:xt-give-x0-eqn-reparm},$t=1,2,\dots,T$.(See eq (14)
    in\cite{ho2020denoising}) and the re-parameterization section in this article\cite{WhatareD38:online}.\par


    How the loss is calculated for each batch of the optimization loop (assuming an iterative optimization algorithm is applied):
    \begin{enumerate}
        \item The parameter values $\bm{\theta}$ are given, either as the initial or the updated values during optimization.
        \item Sample $t$ uniformly from $1,2,\dots T$
        \item Sample $\mathbf{x}(t)$ based on eq. \ref{eq:xt-give-x0-eqn-reparm}
        \item Sample $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
        \item Calculate $\bm{\epsilon}_{\theta}(\mathbf{x}(t),t)$
        \item Calculate the Loss as in eq.(\ref{eq:ddpm-loss-simple})
    \end{enumerate}

    %%%

    \subsection{Modeling $\epsilon_{\theta}(\mathbf{x}(t),t)$}\label{subsec:modeling-ddpm-prametric-noise}
    The core modeling task for Diffusion Models is to, a first design an \textit{ansatz} for
    $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$, then develop an optimization algorithm to find the optimal $\bm{\theta}$
    That minimizes the loss in eq.(\ref{eq:ddpm-loss-simple}).

    In this subsection, we will show the different
    Neural Network architecture used for this ansatz realization.
    Also, we will go through the Positional Time Encoding used to embed time into such a model,

    \subsubsection{Architecture modeling $\epsilon_{\theta}(\mathbf{x}(t),t)$}
    In this section we present different Neural Network architectures applied in the literature for modeling
    $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$ \cite{Diffusio93:online,ho2020denoising,WhatareD38:online}.

    %%%

    \subsubsection{Modeling $\epsilon_{\theta}(\mathbf{x}(t),t)$ with ResNets}
    One of the simple approaches for modeling $\bm{\epsilon}_\bm{\theta}$ is using Res-Net like architecture.
    The code presented\href{https://github.com/Jmkernes/Diffusion/blob/main/diffusion/ddpm/main.py#L101}{here}\cite{Jmkernes71:online}, has
    successfully applied ResNet-like architecture for the density estimation of 2D swissroll dataset\cite{sklearnd99:online}.
    The equation set describing the applied ResNet model of depth $L$ can be written as follows
    (in order from left to right in the model implementation)
    \begin{equation}
        \begin{aligned}
            \mathbf{u}(t) &= \mathbf{x}(t) + \mathbf{g}(t) \\
            \mathbf{z}(0) &= \mathbf{A}_1 \mathbf{u}(0) \quad \textit{Dimension expansion} \\
            \mathbf{a}(l) &= \mathbf{B_2}(\bm{\sigma}(\mathbf{B}_1(\mathbf{z}(l-1)) \quad l=1,2,\dots,L \quad \textit{Non-Linearity}\\
            \mathbf{z}(l) &= \frac{\mathbf{a}(l)-E(\mathbf{a}(l))}{\sqrt {Var(\mathbf{a}(l))+\epsilon}}*\gamma + \beta \quad \textit{Normalization Layer } \\
            \bm{\epsilon}_{\bm{\theta}}(\mathbf{x}(t),t) &=\mathbf{A}_2\mathbf{z}(L) \quad \textit{Dimension contraction}\\
        \end{aligned}\label{eq:ddpm-resnet}
    \end{equation}
    Where $\mathbf{g}(t):\mathbb{Z}^* \rightarrow \mathbb{R}^D$ is the time Positional Encoding function.
    A detailed explanation for this encoding approach is presented in subsection ( \ref{subsubsec:positional-time-encoding}).

% TODO This para is a bit of a clutter !
%    The parameters vector can be written as $\bm{\theta}=[vec(\mathbf{A}_1),vec(\mathbf{A}_2),vec(\mathbf{B}_1),vec(\mathbf{B}_2)),\gamma,\beta]$
%    where $\mathbf{z}_l \in \mathbb{R}^{D_1},\mathbf{A}_1 \in \mathbb{R}^{D \times D_1},\mathbf{A}_2 \in \mathbb{R}^{D_1 \times D},
%    \mathbf{B}_1 \in \mathbb{R}^{D_1 \times D_2},\mathbf{B}_2 \in \mathbb{R}^{D_2 \times D_1}$.


    A more sophisticated model for approximating the parametric noise $\bm{\epsilon}_{\bm{\theta}}(\mathbf{x}(t),t)$ is
    U-Net, which is a Neural Network architecture composed of several ResNet blocks in U shape, and designed
    especially for image segmentation problems\cite{ronneberger2015unet}.
    UNet also applies Positional Encoding for time conditioning \cite{WhatareD38:online}
    This architecture has been used in DDPM \cite{ho2020denoising} and the more generic counterpart of DDPMs: the score-based
    generative models \cite{song2020generative,song2021scorebased,Generati50:online}
    In the next subsection, we provide more details about this architecture.

    \subsubsection{Modeling $\epsilon_{\theta}(\mathbf{x}(t),t)$ with U-Net}
    \textcolor{red}{TBD: \cite{ho2020denoising,sohldickstein2015deep_diffusion_model_main_paper}}

    \subsubsection{Positional Time Embedding}\label{subsubsec:positional-time-encoding}
    Positional Encoding is a time-embedding technique that has been successfully applied in Transformers architectures\cite{lin2021survey}.
    Let $t=0,1,2,\dots,T$ be a time-point in the diffusion process; Positional-Encoding is a function that maps
    the scalar-values time-point $t$ to a vector-valued time-embedding $\mathbf{g}(t)$ based on the position of
    $t$ in the sequence $1,2,\dots,T$.
    Formally, let $let : \mathbf{g}(t)=[g(t;j)]_{j=1}^{d} :\mathbb{Z}^{*} \rightarrow \mathbb{R}$ :
    where $d$ is the dimension of the output embedding
    \begin{equation}
        \begin{aligned}
            g(t; 2 i) & =\sin \left(\frac{t}{n^{2 i / d}}\right) \\
            g(t; 2 i+1) & =\cos \left(\frac{t}{n^{2 i / d}}\right)
        \end{aligned}
        \label{eq:positional_encoding}
    \end{equation}
    Where:
    \begin{itemize}
        \item $t$ is the time steps for the DDPM model, $t= 0,1,2,\dots,T$
        \item $i$ is the index for the project time-embedding dimension $d$ where $i=0,1,2,..,\frac{d}{2} -2$.
        For calculation convenience, $d$ is usually selected to be an even number.
        \item $n$ User defined scalar, usually set to 10,000 as mentioned in
        \item $P(t,j)$ is the projection of the time scalar $t$ to the $j^{th}$ projected dimension where $j=0$
    \end{itemize}

\end{document}