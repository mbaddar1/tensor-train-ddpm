%! Author = mbaddar
%! Date = 4/25/24

% Preamble
\documentclass[11pt]{article}
% Packages


% Document
\begin{document}
    DDPM models are one of the state-of-the-art generative models\cite{ho2020denoising}.
    Given a random variable $\mathbf{x}_0 \in \mathbb{R}^{D} $ with density $q(\mathbf{x}_0)$. The task is to
    find a set of parameters $\bm{\theta}$ for the approximate density function $p(\mathbf{x}_0;\bm{\theta})$
    such that the log-likelihood $ \log(p(\mathbf{x}_0;\bm{\theta}))$ is maximized.

    In DDPM, the training happens in two phases:

    \paragraph{Forward Process}
    In this phase, a set of intermediate latent variables are generated $\mathbf{x}_t \quad t={0,1,2,..,T}$, where
    $\mathbf{x}_t$is the input data and $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})
    \quad \mathbf{0} \in \mathbb{R}^{D}, \mathbf{I} \in \mathbb{R}^{D \times D}$.\par
    Given the following set of constants:

    \begin{equation}
        \begin{aligned}
            \beta_t &, \quad \leq \beta_t \leq 1 \\
            \alpha_t &= 1-\beta_t \\
            \bar{\alpha}_t &= \prod_{i=1}^{T} \alpha_i \\
        \end{aligned}
        \label{eq:ddpm-constants}
    \end{equation}
    The latent variable $\mathbf{x}_t$ has the following distribution:
    \begin{equation}
        \label{eq:ddpm-xt-given-xt-minus-1}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) &=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right) \\
            q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) &=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
        \end{aligned}
    \end{equation}
    \textcolor{red}{TBD : Add details for deriving the forward sampling equation}:
    Which can be rewritten as

    \begin{equation}
        \label{eq:ddpm-xt-given-x0}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
        \end{aligned}
    \end{equation}
    Accordingly, in the forward phase for each time $t=1,2,\dots T$, a corresponding $\mathbf{x}_t$ is generated by
    sampling from density in eq.(\ref{eq:ddpm-xt-given-x0}).
    %%%

    \paragraph{Reverse Process}
    This the process in which we try to train a model the reconstructs $\mathbf{x}^{'}_0 \sim q$ f
    rom normal noise $\mathbf{x}_T$.\par
    \textcolor{red}{Add loss function derivation}\par
    The loss function for DDPM can be written as:
    \begin{equation}
        \begin{aligned}
            L_{\text {simple }}(\theta) =\mathbb{E}_{t, \mathbf{x}_0,
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
            \theta\left(\sqrt{\bar{\alpha}_t}
            \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t}
            \boldsymbol{\epsilon}, t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple}
    \end{equation}
    For $t=1,2,\dots,T$.(See eq (14) in\cite{ho2020denoising})\par.

    How the loss is calculated: For each batch of the optimization, sample $t$ uniformly from $1,2,\dots T$, then compute
    the loss as in eq.(\ref{eq:ddpm-loss-simple}).
\end{document}