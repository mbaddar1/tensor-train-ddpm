%! Author = mbaddar
%! Date = 4/25/24

% Preamble
\documentclass[11pt]{article}
% Packages


% Document
\begin{document}
    DDPM models are one of the state-of-the-art generative models\cite{ho2020denoising}.
    Given a random variable $\mathbf{x}_0 \in \mathbb{R}^{D} $ with density $q(\mathbf{x}_0)$. The task is to
    find a set of parameters $\bm{\theta}$ for the approximate density function $p(\mathbf{x}_0;\bm{\theta})$
    such that the log-likelihood $ \log(p(\mathbf{x}_0;\bm{\theta}))$ is maximized.

    \subsection{Forward and Reverse Process}\label{subsec:forward-and-reverse-process}
    In DDPM, the training happens in two phases:

    \paragraph{Forward Process}
    In this phase, a set of intermediate latent variables are generated $\mathbf{x}_t \quad t={0,1,2,..,T}$, where
    $\mathbf{x}_t$is the input data and $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})
    \quad \mathbf{0} \in \mathbb{R}^{D}, \mathbf{I} \in \mathbb{R}^{D \times D}$.\par
    Given the following set of constants:

    \begin{equation}
        \begin{aligned}
            \beta_t &, \quad \leq \beta_t \leq 1 \\
            \alpha_t &= 1-\beta_t \\
            \bar{\alpha}_t &= \prod_{i=1}^{T} \alpha_i \\
        \end{aligned}
        \label{eq:ddpm-constants}
    \end{equation}
    The latent variable $\mathbf{x}_t$ has the following distribution:
    \begin{equation}
        \label{eq:ddpm-xt-given-xt-minus-1}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) &=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right) \\
            q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) &=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
        \end{aligned}
    \end{equation}
    \textcolor{red}{TBD : Add details for deriving the forward sampling equation}:
    Which can be rewritten as

    \begin{equation}
        \label{eq:ddpm-xt-dist-given-x0}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
        \end{aligned}
    \end{equation}
    Accordingly, in the forward phase for each time $t=1,2,\dots T$, a corresponding $\mathbf{x}_t$ is generated by
    sampling from density in eq.(\ref{eq:ddpm-xt-dist-given-x0}). In another word, $\mathbf{x}_t$ can be sampled using
    the following equation (based on the Re-parameterization trick\cite{kingma2022autoencoding,TheRepar23:online})
    \begin{equation}
        \label{eq:xt-give-x0-eqn-reparm}
        \begin{aligned}
            \mathbf{x}_t &=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}\\
        \end{aligned}
    \end{equation}
    Where $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
    %%%

    \paragraph{Reverse Process}
    This the process in which we try to train a model the reconstructs $\mathbf{x}^{'}_0 \sim q$ f
    rom normal noise $\mathbf{x}_T$.

    \newline
    \textcolor{red}{Add loss function derivation}
    \newline

    The loss function for DDPM, given a sample $\mathbf{x0} \sim q(\mathbf{x}_0)$ can be written as:
    \begin{equation}
        \begin{aligned}
            L_{\text {simple }}(\theta) =\mathbb{E}_{t,\mathbf{x}_t \mid \mathbf{x}_0,
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
            \theta\left(\mathbf{x}_t, t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple}
    \end{equation}
    Where $\mathbf{x}_t$ is generated as in eq. (\ref{eq:xt-give-x0-eqn-reparm}),$t=1,2,\dots,T$.(See eq (14)
    in\cite{ho2020denoising}) and the re-parameterization section in this article \cite{WhatareD38:online}.\par

    How the loss is calculated for each batch of the optimization loop (assuming an iterative optimization algorithm is applied):
    \begin{enumerate}
        \item The parameter values $\bm{\theta}$ are given, either as the initial or the updated values during optimization.
        \item Sample $t$ uniformly from $1,2,\dots T$
        \item Sample $\mathbf{x}_t$ based on eq. \ref{eq:xt-give-x0-eqn-reparm}
        \item Sample $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
        \item Calculate $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$
        \item Calculate the Loss as in eq.(\ref{eq:ddpm-loss-simple})
    \end{enumerate}

    %%%

    \subsection{Modeling $\epsilon_{\theta}(\mathbf{x}_t,t)$}\label{subsec:modeling-ddpm-prametric-noise}
    The core modeling task for Diffusion Models is to, a first design and architecture for a model that approximates
    $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$, then develop an optimization algorithm to find the optimal $\bm{\theta}$
    That minimizes the loss in eq.(\ref{eq:ddpm-loss-simple}).

    In this subsection, we will show the different
    Neural Network architecture used for different realizations of Diffusion Models and how they are optimized.
    Also, we will go through the Positional Time Encoding used to embed time into such a model,

    \subsubsection{Architecture modeling $\epsilon_{\theta}(\mathbf{x}_t,t)$}
    In this section we present different Neural Network architectures applied in the literature for modeling
    $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$ \cite{Diffusio93:online,ho2020denoising,WhatareD38:online}.

    %%%

    \subsubsection{Modeling $\epsilon_{\theta}(\mathbf{x}_t,t)$ with ResNets}
    One of the simple approaches for modeling $\bm{\epsilon}_\bm{\theta}$ is using Res-Net like architecture.
    The code presented \href{https://github.com/Jmkernes/Diffusion/blob/main/diffusion/ddpm/main.py#L101}{here}, has
    successfully applied ResNet-like architecture for the density estimation of 2D swissroll dataset \cite{sklearnd99:online}.
    The equation set describing the applied ResNet model of depth $L$ can be written as follows
    (in order from left to right in the model implementation)
    \begin{equation}
        \begin{aligned}
            \mathbf{x}_t^{'} &= \mathbf{x}_t + t^{'} \\
            \mathbf{z}_0 &= \mathbf{A}_1 \mathbf{x}^{'} \quad \text{Dimension expansion} \\
            \mathbf{u} &= \mathbf{B_2}(\bm{\sigma}(\mathbf{B}_1(\mathbf{z}_{l-1})) \quad l=1,2,\dots,L \quad \text{Non-Linearity}\\
            \mathbf{z}_l &= \frac{\mathbf{u}-E(\mathbf{u})}{\sqrt {Var(\mathbf{u})+\epsilon}}*\gamma + \beta \quad \text{Normalization Layer } \\
            \bm{\epsilon}_{\bm{\theta}}(\mathbf{x}_t,t) &=\mathbf{A}_2\mathbf{z}_L \quad \text{Dimension contraction}\\
        \end{aligned}\label{eq:ddpm-resnet}
    \end{equation}
    Where $t^{'}$ is encoded time based on the Positional Encoding function $f:\mathbb{R} \rightarrow \mathbb{R}^{D}$
    (A detailed explanation for this encoding approach is presented in subsection ( \ref{subsubsec:positional-time-encoding}).


    The parameters vector can be written as $\bm{\theta}=[vec(\mathbf{A}_1),vec(\mathbf{A}_2),vec(\mathbf{B}_1),vec(\mathbf{B}_2)),\gamma,\beta]$
    where $\mathbf{z}_l \in \mathbb{R}^{D_1},\mathbf{A}_1 \in \mathbb{R}^{D \times D_1},\mathbf{A}_2 \in \mathbb{R}^{D_1 \times D},
    \mathbf{B}_1 \in \mathbb{R}^{D_1 \times D_2},\mathbf{B}_2 \in \mathbb{R}^{D_2 \times D_1}$.


    A more sophisticated model for approximating the parametric noise $\bm{\epsilon}_{\bm{\theta}}(\mathbf{x}_t,t)$ is
    U-Net, which is a Neural Network architecture composed of several ResNet blocks in U shape, and designed
    especially for image segmentation problems\cite{ronneberger2015unet}.
    UNet also applies Positional Encoding for time conditioning \cite{WhatareD38:online}
    This architecture has been used in DDPM \cite{ho2020denoising} and the more generic counterpart of DDPMs: the score-based
    generative models \cite{song2020generative,song2021scorebased,Generati50:online}
    In the next subsection, we provide more details about this architecture.

    \subsubsection{Modeling $\epsilon_{\theta}(\mathbf{x}_t,t)$ with U-Net}
    \textcolor{red}{TBD: \cite{ho2020denoising,sohldickstein2015deep_diffusion_model_main_paper}}

    \subsubsection{Positional Time Embedding}\label{subsubsec:positional-time-encoding}
    \textcolor{red}{TBD : \cite{Demystif18:online,AGentleI11:online,vaswani2023attention}}


\end{document}