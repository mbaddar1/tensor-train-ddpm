%! Author = mbaddar
%! Date = 4/25/24

% Preamble
\documentclass[11pt]{article}
% Packages


% Document
\begin{document}
    DDPM models are one of the state-of-the-art generative models\cite{ho2020denoising}.
    Given a random variable $\mathbf{x}_0 \in \mathbb{R}^{D} $ with density $q(\mathbf{x}_0)$. The task is to
    find a set of parameters $\bm{\theta}$ for the approximate density function $p(\mathbf{x}_0;\bm{\theta})$
    such that the log-likelihood $ \log(p(\mathbf{x}_0;\bm{\theta}))$ is maximized.

    \subsection{Forward and Reverse Process}\label{subsec:forward-and-reverse-process}
    In DDPM, the training happens in two phases:

    \paragraph{Forward Process}
    In this phase, a set of intermediate latent variables are generated $\mathbf{x}_t \quad t={0,1,2,..,T}$, where
    $\mathbf{x}_t$is the input data and $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0},\mathbf{I})
    \quad \mathbf{0} \in \mathbb{R}^{D}, \mathbf{I} \in \mathbb{R}^{D \times D}$.\par
    Given the following set of constants:

    \begin{equation}
        \begin{aligned}
            \beta_t &, \quad \leq \beta_t \leq 1 \\
            \alpha_t &= 1-\beta_t \\
            \bar{\alpha}_t &= \prod_{i=1}^{T} \alpha_i \\
        \end{aligned}
        \label{eq:ddpm-constants}
    \end{equation}
    The latent variable $\mathbf{x}_t$ has the following distribution:
    \begin{equation}
        \label{eq:ddpm-xt-given-xt-minus-1}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right) &=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right) \\
            q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) &=\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
        \end{aligned}
    \end{equation}
    \textcolor{red}{TBD : Add details for deriving the forward sampling equation}:
    Which can be rewritten as

    \begin{equation}
        \label{eq:ddpm-xt-dist-given-x0}
        \begin{aligned}
            q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ;
            \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
        \end{aligned}
    \end{equation}
    Accordingly, in the forward phase for each time $t=1,2,\dots T$, a corresponding $\mathbf{x}_t$ is generated by
    sampling from density in eq.(\ref{eq:ddpm-xt-dist-given-x0}). In another word, $\mathbf{x}_t$ can be sampled using
    the following equation (based on the Re-parameterization trick\cite{kingma2022autoencoding,TheRepar23:online})
    \begin{equation}
        \label{eq:xt-give-x0-eqn-reparm}
        \begin{aligned}
            \mathbf{x}_t &=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}\\
        \end{aligned}
    \end{equation}
    Where $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
    %%%

    \paragraph{Reverse Process}
    This the process in which we try to train a model the reconstructs $\mathbf{x}^{'}_0 \sim q$ f
    rom normal noise $\mathbf{x}_T$.

    \newline
    \textcolor{red}{Add loss function derivation}
    \newline

    The loss function for DDPM can be written as:
    \begin{equation}
        \begin{aligned}
            L_{\text {simple }}(\theta) =\mathbb{E}_{t, \mathbf{x}_0,
            \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_
            \theta\left(\mathbf{x}_t, t\right)\right\|^2\right]
        \end{aligned}
        \label{eq:ddpm-loss-simple}
    \end{equation}
    Where $\mathbf{x}_t$ is generated as in eq. (\ref{eq:xt-give-x0-eqn-reparm}),$t=1,2,\dots,T$.(See eq (14)
    in\cite{ho2020denoising}) and the re-parameterization section in this article \cite{WhatareD38:online}.\par

    How the loss is calculated for each batch of the optimization loop (assuming an iterative optimization algorithm is applied):
    \begin{enumerate}
        \item The parameter values $\bm{\theta}$ are given, either as the initial or the updated values during optimization.
        \item Sample $t$ uniformly from $1,2,\dots T$
        \item Sample $\mathbf{x}_t$ based on eq. \ref{eq:xt-give-x0-eqn-reparm}
        \item Sample $\bm{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$
        \item Calculate $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$
        \item Calculate the Loss as in eq.(\ref{eq:ddpm-loss-simple})
    \end{enumerate}
    
    %%%
    \subsection{Modeling $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$}\label{subsec:modeling-ddpm-prametric-noise}
    The core modeling task for Diffusion Models is to, a first design and architecture for a model that approximates
    $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$, then develop an optimization algorithm to find the optimal $\bm{\theta}$
    That minimizes the loss in eq.(\ref{eq:ddpm-loss-simple}).

    In this subsection, we will show the different
    Neural Network architecture used for different realizations of Diffusion Models and how they are optimized.
    Also, we will go through the Positional Time Encoding used to embed time into such a model,

    \subsubsection{Positional Time Embedding}
    \textcolor{red}{TBD : \cite{Demystif18:online,AGentleI11:online,vaswani2023attention}}

    \subsubsection{ResNet and UNet for modeling $\bm{\epsilon}_{\theta}(\mathbf{x}_t,t)$}
    \textcolor{red}{TBD: \cite{ho2020denoising,sohldickstein2015deep_diffusion_model_main_paper}}

\end{document}