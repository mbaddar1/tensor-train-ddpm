LOSS LANDSCAPES ARE ALL YOU NEED: NEURAL NETWORK GENERALIZATION CAN BE EXPLAINED WITHOUT THE IMPLICIT BIAS OF GRADIENT DESCENT
https://openreview.net/pdf?id=QC10RmRbZy9

OPTIMIZATION METHODS ON RIEMANNIAN MANIFOLDS AND THEIR APPLICATION TO SHAPE SPACE
https://www.uni-muenster.de/AMM/num/wirth/files/RiWi12.pdf
"Even if an embedding is known, one might hope that a
Riemannian optimization method performs more efficiently since it exploits the underlying geometric structure of the
manifold. For this purpose, various methods have been devised, from simple gradient descent on manifolds [25] to
sophisticated trust region methods [5]."

Visualizing the Loss Landscape of Neural Nets
https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf

A Note on the Convergence of Denoising Diffusion Probabilistic Models
https://openreview.net/pdf?id=wLe1bG93yc

Convergence of score-based generative modeling for general data distributions
https://proceedings.mlr.press/v201/lee23a/lee23a.pdf

====
Tensor Train Optimization Resources
---------
On the closedness of the TT format: http://link.springer.com/10.1007/s00791-012-0183-y
The manifold of fixed TT-ranks tensors: https://link.springer.com/article/10.1007/s00211-011-0419-7
On the convergence of the ALS algorithm: http://epubs.siam.org/doi/10.1137/110857520 and http://arxiv.org/abs/1506.00062
Riemannian Optimization for TT: http://epubs.siam.org/doi/10.1137/15M1010506


