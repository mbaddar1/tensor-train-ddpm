%! Author = mbaddar
%! Date = 4/25/24

@misc{ho2020denoising,
    title = {Denoising Diffusion Probabilistic Models},
    author = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
    year = {2020},
    eprint = {2006.11239},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{WhatareD38:online,
    author = {},
    title = {What are Diffusion Models? | Lil'Log},
    howpublished = {\url{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}},
    month = {},
    year = {},
    note = {(Accessed on 04/25/2024)}
}

@misc{sohldickstein2015deep_diffusion_model_main_paper,
    title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
    author = {Jascha Sohl-Dickstein and Eric A. Weiss and Niru Maheswaranathan and Surya Ganguli},
    year = {2015},
    eprint = {1503.03585},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{kingma2022autoencoding,
    title = {Auto-Encoding Variational Bayes},
    author = {Diederik P Kingma and Max Welling},
    year = {2022},
    eprint = {1312.6114},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@misc{TheRepar23:online,
    author = {},
    title = {The Reparameterization Trick},
    howpublished = {\url{https://gregorygundersen.com/blog/2018/04/29/reparameterization/#kingma2013auto}},
    month = {},
    year = {},
    note = {(Accessed on 04/26/2024)}
}

@misc{Demystif18:online,
    author = {},
    title = {Demystifying Diffusion Models. Generative models learns a… | by Rucha Apte | Medium},
    howpublished = {\url{https://ruchaa.medium.com/demystifying-diffusion-models-c7e0689a7ca8}},
    month = {},
    year = {},
    note = {(Accessed on 04/26/2024)}
}

@misc{AGentleI11:online,
    author = {},
    title = {A Gentle Introduction to Positional Encoding in Transformer Models, Part 1 - MachineLearningMastery.com},
    howpublished = {\url{https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/}},
    month = {},
    year = {},
    note = {(Accessed on 04/26/2024)}
}

@misc{vaswani2023attention,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year = {2023},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}

@misc{Diffusio93:online,
    author = {},
    title = {Diffusion/diffusion/ddpm at main · Jmkernes/Diffusion},
    howpublished = {\url{https://github.com/Jmkernes/Diffusion/tree/main/diffusion/ddpm}},
    month = {},
    year = {},
    note = {(Accessed on 04/27/2024)}
}

@misc{sklearnd99:online,
    author = {},
    title = {sklearn.datasets.make\_swiss\_roll — scikit-learn 1.4.2 documentation},
    howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html}},
    month = {},
    year = {},
    note = {(Accessed on 04/27/2024)}
}

@misc{ronneberger2015unet,
    title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
    author = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
    year = {2015},
    eprint = {1505.04597},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}

@misc{song2021scorebased,
    title = {Score-Based Generative Modeling through Stochastic Differential Equations},
    author = {Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
    year = {2021},
    eprint = {2011.13456},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{song2020generative,
    title = {Generative Modeling by Estimating Gradients of the Data Distribution},
    author = {Yang Song and Stefano Ermon},
    year = {2020},
    eprint = {1907.05600},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{Generati50:online,
    author = {Yang Song},
    title = {Generative Modeling by Estimating Gradients of the Data Distribution | Yang Song},
    howpublished = {\url{https://yang-song.net/blog/2021/score/}},
    month = {5},
    year = {2021},
    note = {(Accessed on 04/27/2024)}
}

@article{Kolda2009TensorDA,
    title = {Tensor Decompositions and Applications},
    author = {Tamara G. Kolda and Brett W. Bader},
    journal = {SIAM Rev.},
    year = {2009},
    volume = {51},
    pages = {455-500},
    url = {https://api.semanticscholar.org/CorpusID:16074195}
}

@misc{qi2022exploiting,
    title = {Exploiting Low-Rank Tensor-Train Deep Neural Networks Based on Riemannian Gradient Descent With Illustrations of Speech Processing},
    author = {Jun Qi and Chao-Han Huck Yang and Pin-Yu Chen and Javier Tejedor},
    year = {2022},
    eprint = {2203.06031},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{sommer2024generative,
    title = {Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations},
    author = {David Sommer and Robert Gruhlke and Max Kirstein and Martin Eigel and Claudia Schillings},
    year = {2024},
    eprint = {2402.15285},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@misc{novikov2022tensortrain,
    title = {Tensor-Train Density Estimation},
    author = {Georgii S. Novikov and Maxim E. Panov and Ivan V. Oseledets},
    year = {2022},
    eprint = {2108.00089},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{han2024tensor,
    title = {Tensor train based sampling algorithms for approximating regularized Wasserstein proximal operators},
    author = {Fuqun Han and Stanley Osher and Wuchen Li},
    year = {2024},
    eprint = {2401.13125},
    archivePrefix = {arXiv},
    primaryClass = {math.OC}
}

@article{article,
    author = {Oseledets, Ivan},
    year = {2011},
    month = {01},
    pages = {},
    title = {DMRG approach to fast linear algebra in the TT-format},
    volume = {11},
    journal = {Computational Methods in Applied Mathematics},
    doi = {10.2478/cmam-2011-0021}
}

@article{Gorodetsky_2018,
    title = {Gradient-based optimization for regression in the functional tensor-train format},
    volume = {374},
    ISSN = {0021-9991},
    url = {http://dx.doi.org/10.1016/j.jcp.2018.08.010},
    DOI = {10.1016/j.jcp.2018.08.010},
    journal = {Journal of Computational Physics},
    publisher = {Elsevier BV},
    author = {Gorodetsky, Alex A. and Jakeman, John D.},
    year = {2018},
    month = dec, pages = {1219–1238} }

@article{doi:10.1137/100818893,
    author = {Holtz, Sebastian and Rohwedder, Thorsten and Schneider, Reinhold},
    title = {The Alternating Linear Scheme for Tensor Optimization in the Tensor Train Format},
    journal = {SIAM Journal on Scientific Computing},
    volume = {34},
    number = {2},
    pages = {A683-A713},
    year = {2012},
    doi = {10.1137/100818893},
    URL = {
    https://doi.org/10.1137/100818893
    },
    eprint = {
    https://doi.org/10.1137/100818893
    }
    ,
    abstract = { Recent achievements in the field of tensor product approximation provide promising new formats for the representation of tensors in form of tree tensor networks. In contrast to the canonical r-term representation (CANDECOMP, PARAFAC), these new formats provide stable representations, while the amount of required data is only slightly larger. The tensor train (TT) format [SIAM J. Sci. Comput., 33 (2011), pp. 2295–2317], a simple special case of the hierarchical Tucker format [J. Fourier Anal. Appl., 5 (2009), p. 706], is a useful prototype for practical low-rank tensor representation. In this article, we show how optimization tasks can be treated in the TT format by a generalization of the well-known alternating least squares (ALS) algorithm and by a modified approach (MALS) that enables dynamical rank adaptation. A formulation of the component equations in terms of so-called retraction operators helps to show that many structural properties of the original problems transfer to the micro-iterations, giving what is to our knowledge the first stable generic algorithm for the treatment of optimization tasks in the tensor format. For the examples of linear equations and eigenvalue equations, we derive concrete working equations for the micro-iteration steps; numerical examples confirm the theoretical results concerning the stability of the TT decomposition and of ALS and MALS but also show that in some cases, high TT ranks are required during the iterative approximation of low-rank tensors, showing some potential of improvement. }
}

@article{GORODETSKY20181219,
    title = {Gradient-based optimization for regression in the functional tensor-train format},
    journal = {Journal of Computational Physics},
    volume = {374},
    pages = {1219-1238},
    year = {2018},
    issn = {0021-9991},
    doi = {https://doi.org/10.1016/j.jcp.2018.08.010},
    url = {https://www.sciencedirect.com/science/article/pii/S0021999118305321},
    author = {Alex A. Gorodetsky and John D. Jakeman},
    keywords = {Tensors, Regression, Function approximation, Uncertainty quantification, Alternating least squares, Stochastic gradient descent},
    abstract = {Predictive analysis of complex computational models, such as uncertainty quantification (UQ), must often rely on using an existing database of simulation runs. In this paper we consider the task of performing low-multilinear-rank regression on such a database. Specifically we develop and analyze an efficient gradient computation that enables gradient-based optimization procedures, including stochastic gradient descent and quasi-Newton methods, for learning the parameters of a functional tensor-train (FT). We compare our algorithms with 22 other nonparametric and parametric regression methods on 10 real-world data sets and show that for many physical systems, exploiting low-rank structure facilitates efficient construction of surrogate models. We use a number of synthetic functions to build insight into behavior of our algorithms, including the rank adaptation and group-sparsity regularization procedures that we developed to reduce overfitting. Finally we conclude the paper by building a surrogate of a physical ftt_ddpm_model of a propulsion plant on a naval vessel.}
}

@article{Klus_2019,
    title = {Tensor-Based Algorithms for Image Classification},
    volume = {12},
    ISSN = {1999-4893},
    url = {http://dx.doi.org/10.3390/a12110240},
    DOI = {10.3390/a12110240},
    number = {11},
    journal = {Algorithms},
    publisher = {MDPI AG},
    author = {Klus, Stefan and Gelß, Patrick},
    year = {2019},
    month = nov, pages = {240} }

@misc{stoudenmire2017supervised,
    title = {Supervised Learning with Quantum-Inspired Tensor Networks},
    author = {E. Miles Stoudenmire and David J. Schwab},
    year = {2017},
    eprint = {1605.05775},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@misc{Calculus40:online,
    author = {Paul},
    title = {Calculus III - Vector Functions},
    howpublished = {\url{https://tutorial.math.lamar.edu/classes/calciii/VectorFunctions.aspx}},
    month = {},
    year = {},
    note = {(Accessed on 05/02/2024)}
}

@misc{lin2021survey,
    title = {A Survey of Transformers},
    author = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
    year = {2021},
    eprint = {2106.04554},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{Anovervi66:online,
    author = {},
    title = {An overview of Gaussian Mixture Models},
    howpublished = {\url{https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html}},
    month = {},
    year = {},
    note = {(Accessed on 05/02/2024)}
}

@misc{Ch92Mixt66:online,
    author = {Sargur Srihari },
    title = {Mixtures of Gaussians },
    howpublished = {\url{https://cedar.buffalo.edu/~srihari/CSE574/Chap9/Ch9.2-MixturesofGaussians.pdf}},
    month = {},
    year = {},
    note = {(Accessed on 05/04/2024)}
}

@misc{APIRefer67:online,
    author = {},
    title = {sklearn Datasets module},
    howpublished = {\url{https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets}},
    month = {},
    year = {},
    note = {(Accessed on 05/04/2024)}
}

@misc{sklearnd40:online,
    author = {},
    title = {sklearn digits module},
    howpublished = {\url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits}},
    month = {},
    year = {},
    note = {(Accessed on 05/04/2024)}
}

@misc{papamakarios2018masked,
    title = {Masked Autoregressive Flow for Density Estimation},
    author = {George Papamakarios and Theo Pavlakou and Iain Murray},
    year = {2018},
    eprint = {1705.07057},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}

@misc{Datasets50:online,
    author = {},
    title = {Datasets - UCI Machine Learning Repository},
    howpublished = {\url{https://archive.ics.uci.edu/datasets}},
    month = {},
    year = {},
    note = {(Accessed on 05/04/2024)}
}

@misc{misc_individual_household_electric_power_consumption_235,
    author = {Hebrail,Georges and Berard,Alice},
    title = {{Individual Household Electric Power Consumption}},
    year = {2012},
    howpublished = {UCI Machine Learning Repository},
    note = {{DOI}: https://doi.org/10.24432/C58K54}
}

@misc{misc_hepmass_347,
    author = {Whiteson,Daniel},
    title = {{HEPMASS}},
    year = {2016},
    howpublished = {UCI Machine Learning Repository},
    note = {{DOI}: https://doi.org/10.24432/C5PP5W}
}
%@misc{Indexofd14:online,
%author = {},
%title = {HEPMASS datafiles },
%howpublished = {\url{https://mlphysics.ics.uci.edu/data/hepmass/}},
%month = {},
%year = {},
%note = {(Accessed on 05/04/2024)}
%}
@misc{UCIHEPMA22:online,
author = {},
title = {UCI HEPMASS Benchmark (Density Estimation) | Papers With Code},
howpublished = {\url{https://paperswithcode.com/sota/density-estimation-on-uci-hepmass}},
month = {},
year = {},
note = {(Accessed on 05/04/2024)}
}
@misc{bigdeli2020learning,
      title={Learning Generative Models using Denoising Density Estimators},
      author={Siavash A. Bigdeli and Geng Lin and Tiziano Portenier and L. Andrea Dunbar and Matthias Zwicker},
      year={2020},
      eprint={2001.02728},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{misc_miniboone_particle_identification_199,
  author       = {Roe,Byron},
  title        = {{MiniBooNE particle identification}},
  year         = {2010},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5QC87}
}
@article{Panaretos_2019,
   title={Statistical Aspects of Wasserstein Distances},
   volume={6},
   ISSN={2326-831X},
   url={http://dx.doi.org/10.1146/annurev-statistics-030718-104938},
   DOI={10.1146/annurev-statistics-030718-104938},
   number={1},
   journal={Annual Review of Statistics and Its Application},
   publisher={Annual Reviews},
   author={Panaretos, Victor M. and Zemel, Yoav},
   year={2019},
   month=mar, pages={405–431} }
@misc{Optpdf94:online,
author = {},
title = {Optimal Transport and Wasserstein Distance},
howpublished = {\url{https://www.stat.cmu.edu/~larry/=sml/Opt.pdf}},
month = {},
year = {},
note = {(Accessed on 05/07/2024)}
}
@inproceedings{NIPS2013_af21d0c9,
 author = {Cuturi, Marco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
 volume = {26},
 year = {2013}
}
@misc{POTPytho87:online,
author = {},
title = {POT: Python Optimal Transport — POT Python Optimal Transport 0.9.3 documentation},
howpublished = {\url{https://pythonot.github.io/}},
month = {},
year = {},
note = {(Accessed on 05/07/2024)}
}
@article{JMLR:v22:20-451,
  author  = {RÃ©mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and AurÃ©lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and LÃ©o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
  title   = {POT: Python Optimal Transport},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {78},
  pages   = {1--8},
  url     = {http://jmlr.org/papers/v22/20-451.html}
}
@misc{Geometri81:online,
author = {},
title = {GeomLoss: Geometric Loss functions between sampled measures, images and volumes — GeomLoss},
howpublished = {\url{https://www.kernel-operations.io/geomloss/}},
month = {},
year = {},
note = {(Accessed on 05/07/2024)}
}
@misc{Jmkernes71:online,
author = {},
title = {Jmkernes/Diffusion: Everything related to diffusion models!},
howpublished = {\url{https://github.com/Jmkernes/Diffusion/tree/main}},
month = {},
year = {},
note = {(Accessed on 05/07/2024)}
}